{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e44716",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- solve the issue of invoking pkl too many times\n",
    "- parametrize the \"overlapping interval\", from 1 second to a bit more\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837670e",
   "metadata": {},
   "source": [
    "## Object criticality for better and safer navigation - GOAL 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c033d59",
   "metadata": {},
   "source": [
    "This notebook is the iterative version, to compute GOAL3 on multiple configurations.\n",
    "\n",
    "\n",
    "Can be executed straighforwardly, after setting some paths. But to really understand what it does, the non-iterative version is recommended. It has many step-by-step comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044e74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import itertools\n",
    "import unittest\n",
    "import sklearn\n",
    "import tqdm\n",
    "import json, os\n",
    "from glob import glob\n",
    "import pandas\n",
    "import math\n",
    "import json\n",
    "from operator import itemgetter\n",
    "from typing import Callable\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.eval.prediction.splits import *\n",
    "import nuscenes.eval.detection.config as cnfig\n",
    "from nuscenes.eval.detection.configs import *\n",
    "from nuscenes.eval.detection.data_classes import DetectionBox \n",
    "from nuscenes.eval.detection import *\n",
    "import nuscenes.eval.detection.algo as ag\n",
    "from nuscenes.eval.detection.data_classes import DetectionMetricData, DetectionConfig, DetectionMetrics, DetectionBox, \\\n",
    "    DetectionMetricDataList\n",
    "from nuscenes.eval.common.data_classes import EvalBoxes\n",
    "from typing import List, Dict, Callable, Tuple\n",
    "from nuscenes.eval.common.utils import center_distance, scale_iou, yaw_diff, velocity_l2, attr_acc, cummean\n",
    "import nuscenes.eval.detection.evaluate as dcl    \n",
    "from nuscenes.prediction import *\n",
    "from nuscenes.map_expansion.map_api import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pkl.models import compile_model\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import operator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224b85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samp2mapname(samp, nusc):\n",
    "    scene = nusc.get('scene', samp['scene_token'])\n",
    "    log = nusc.get('log', scene['log_token'])\n",
    "    return log['location']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d2f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(point_cloud_range, voxel_size):\n",
    "    lower = np.array(point_cloud_range[:(len(point_cloud_range) // 2)])\n",
    "    upper = np.array(point_cloud_range[(len(point_cloud_range) // 2):])\n",
    "\n",
    "    dx = np.array(voxel_size)\n",
    "    bx = lower + dx/2.0\n",
    "    nx = ((upper - lower) / dx).astype(int)\n",
    "\n",
    "    return dx, bx, nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0827c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the trajectory as computed by pkl, in number_of_trajectory_poses frames.\n",
    "#default is 16 frames (one each 0.25 seconds, for a total of 3 seconds)\n",
    "#TRAJ_POINTS is the number of predicted trajectory positions (trajectory points) for each frame\n",
    "def computetrajectory(sT, number_of_trajectory_poses=16, TRAJ_POINTS=25):\n",
    "    dimension=sT[0][0].shape\n",
    "    basic_shape=np.zeros(dimension)    \n",
    "    trajectory=basic_shape\n",
    "    for i in range(0,number_of_trajectory_poses):\n",
    "        #get 20 most relevant points in sT[i][0].cpu().detach().numpy(), i.e., per frame\n",
    "        tmp=sT[i][0].cpu().detach().numpy().copy()\n",
    "        tmp_most_relevant_index=np.argsort(tmp.flatten())[-TRAJ_POINTS:]\n",
    "        tmp.fill(0)\n",
    "        flat=tmp.flatten()\n",
    "        flat[tmp_most_relevant_index]=1\n",
    "        tmp=np.reshape(flat, dimension)\n",
    "        trajectory=np.add(trajectory,tmp)\n",
    "        trajectory[trajectory>0]=1\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "#as computetrajectory, but returns the trajectory between 2 of the trajectory frames\n",
    "def computetrajectory_min_max(sT, minimum=0, maximum=16, number_of_trajectory_poses=16, TRAJ_POINTS=25):\n",
    "    dimension=sT[0][0].cpu().detach().numpy().shape\n",
    "    basic_shape=np.zeros(dimension)    \n",
    "    trajectory=basic_shape\n",
    "    for i in range(minimum,maximum):\n",
    "        #get 20 most relevant points in sT[i][0].cpu().detach().numpy(), i.e., per frame\n",
    "        tmp=sT[i][0].cpu().detach().numpy().copy()\n",
    "        tmp_most_relevant_index=np.argsort(tmp.flatten())[-TRAJ_POINTS:]\n",
    "        tmp.fill(0)\n",
    "        flat=tmp.flatten()\n",
    "        flat[tmp_most_relevant_index]=1\n",
    "        tmp=np.reshape(flat, dimension)\n",
    "        trajectory=np.add(trajectory,tmp)\n",
    "        trajectory[trajectory>0]=1\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c31607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from 1 layer image to RGB all black \n",
    "def toblack(image):\n",
    "    black_image=np.ones((image.shape[0],image.shape[1],3))\n",
    "    black_image[:,:,0][image>0]=0\n",
    "    black_image[:,:,1][image>0]=0\n",
    "    black_image[:,:,2][image>0]=0\n",
    "    return black_image\n",
    "\n",
    "def tored(image):\n",
    "    red_image=np.ones((image.shape[0],image.shape[1],3))\n",
    "#    red_image[:,:,0][image==0]=1\n",
    "    red_image[:,:,1]=red_image[:,:,1]-image\n",
    "    red_image[:,:,2]=red_image[:,:,1]-image\n",
    "    return red_image\n",
    "    \n",
    "def toblu(image):\n",
    "    blu_image=np.ones((image.shape[0],image.shape[1],3))\n",
    "    blu_image[:,:,0]=blu_image[:,:,0]-image\n",
    "    blu_image[:,:,1]=blu_image[:,:,1]-image\n",
    "#    blu_image[:,:,2][image==0]=1\n",
    "    return blu_image\n",
    "\n",
    "    \n",
    "def printfigure3x2(font, t_gt, t_pt, t_dt, gt, original_dt,dynamic_dt, ego):\n",
    "    image_gt=np.ones((original_dt.shape[0],original_dt.shape[1],3))\n",
    "    image_pt=np.ones((original_dt.shape[0],original_dt.shape[1],3))\n",
    "    image_dt=np.ones((original_dt.shape[0],original_dt.shape[1],3))\n",
    "    \n",
    "    ego=tored(ego)\n",
    "    t_gt=toblack(t_gt)\n",
    "    t_pt=toblack(t_pt)\n",
    "    t_dt=toblack(t_dt)\n",
    "\n",
    "    gt=toblu(gt)\n",
    "    original_dt=toblu(original_dt)\n",
    "    dynamic_dt=toblu(dynamic_dt)\n",
    "    plt.figure()\n",
    "    \n",
    "    fig, (ax) = plt.subplots(2,3)\n",
    "    ax[0,0].tick_params(axis='both', labelsize=font)\n",
    "    ax[0,1].tick_params(axis='both', labelsize=font)\n",
    "    ax[0,2].tick_params(axis='both', labelsize=font)\n",
    "    ax[1,0].tick_params(axis='both', labelsize=font)\n",
    "    ax[1,1].tick_params(axis='both', labelsize=font)\n",
    "    ax[1,2].tick_params(axis='both', labelsize=font)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax[0,0].set_title('GT Traj', fontsize=font)\n",
    "    ax[0,0].imshow(np.swapaxes(t_gt, 0, 1))\n",
    "\n",
    "    ax[0,1].set_title('Original Traj', fontsize=font)\n",
    "    ax[0,1].imshow(np.swapaxes(t_pt, 0, 1))\n",
    "\n",
    "    ax[0,2].set_title('Dynamic Traj', fontsize=font)\n",
    "    ax[0,2].imshow(np.swapaxes(t_dt, 0, 1))\n",
    "\n",
    "    ax[1,0].set_title('GT BBox and GT Traj', fontsize=font)\n",
    "    image_gt[t_gt==0]=0\n",
    "    image_gt[gt==0]=0\n",
    "    image_gt[ego==0]=0\n",
    "    image_gt[image_gt>1]=1\n",
    "    ax[1,0].imshow(np.swapaxes(image_gt, 0, 1))\n",
    "\n",
    "    ax[1,1].set_title('Orig BBox and orig pkl', fontsize=font)\n",
    "    image_pt[t_pt==0]=0\n",
    "    image_pt[original_dt==0]=0\n",
    "    image_pt[ego==0]=0\n",
    "    image_pt[image_pt>1]=1\n",
    "    ax[1,1].imshow(np.swapaxes(image_pt, 0, 1))\n",
    "\n",
    "    ax[1,2].set_title('Dyn BBox and dyn pkl', fontsize=font)\n",
    "    image_dt[t_dt==0]=0\n",
    "    image_dt[dynamic_dt==0]=0\n",
    "    image_dt[ego==0]=0\n",
    "    image_dt[image_dt>1]=1\n",
    "    ax[1,2].imshow(np.swapaxes(image_dt, 0, 1))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def printfigure3x1(font, overlap, gt_with_respect_to_ego, \\\n",
    "                   trajectory_next_second, resulting_image):\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    fig, (ax) = plt.subplots(1,3)\n",
    "    ax[0].tick_params(axis='both', labelsize=font)\n",
    "    ax[1].tick_params(axis='both', labelsize=font)\n",
    "    ax[2].tick_params(axis='both', labelsize=font)\n",
    "    fig.tight_layout()\n",
    "        \n",
    "    ax[0].set_title('GT BBs', fontsize=font)\n",
    "    ax[0].imshow(np.swapaxes(gt_with_respect_to_ego, 0, 1))\n",
    "\n",
    "    ax[1].set_title('Predict Traj', fontsize=font)\n",
    "    ax[1].imshow(np.swapaxes(trajectory_next_second, 0, 1))\n",
    "\n",
    "    ax[2].set_title('overlap: {}'.format(overlap), fontsize=font)\n",
    "    ax[2].imshow(np.swapaxes(resulting_image, 0, 1))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd21a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_map(nmap, center, stretch, layer_names, line_names):\n",
    "    # need to get the map here...\n",
    "    box_coords = (\n",
    "        center[0] - stretch,\n",
    "        center[1] - stretch,\n",
    "        center[0] + stretch,\n",
    "        center[1] + stretch,\n",
    "    )\n",
    "\n",
    "    polys = {}\n",
    "\n",
    "    # polygons\n",
    "    records_in_patch = nmap.get_records_in_patch(box_coords,\n",
    "                                                 layer_names=layer_names,\n",
    "                                                 mode='intersect')\n",
    "    for layer_name in layer_names:\n",
    "        polys[layer_name] = []\n",
    "        for token in records_in_patch[layer_name]:\n",
    "            poly_record = nmap.get(layer_name, token)\n",
    "            if layer_name == 'drivable_area':\n",
    "                polygon_tokens = poly_record['polygon_tokens']\n",
    "            else:\n",
    "                polygon_tokens = [poly_record['polygon_token']]\n",
    "\n",
    "            for polygon_token in polygon_tokens:\n",
    "                polygon = nmap.extract_polygon(polygon_token)\n",
    "                polys[layer_name].append(np.array(polygon.exterior.xy).T)\n",
    "\n",
    "    # lines\n",
    "    for layer_name in line_names:\n",
    "        polys[layer_name] = []\n",
    "        for record in getattr(nmap, layer_name):\n",
    "            token = record['token']\n",
    "\n",
    "            line = nmap.extract_line(record['line_token'])\n",
    "            if line.is_empty:  # Skip lines without nodes\n",
    "                continue\n",
    "            xs, ys = line.xy\n",
    "\n",
    "            polys[layer_name].append(\n",
    "                np.array([xs, ys]).T\n",
    "                )\n",
    "\n",
    "    # convert to local coordinates in place\n",
    "    rot = get_rot(np.arctan2(center[3], center[2])).T\n",
    "    for layer_name in polys:\n",
    "        for rowi in range(len(polys[layer_name])):\n",
    "            polys[layer_name][rowi] -= center[:2]\n",
    "            polys[layer_name][rowi] = np.dot(polys[layer_name][rowi], rot)\n",
    "\n",
    "    return polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3fdc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the samples that follows my_selected_sample, for the defined seconds\n",
    "def get_following_tokens(seconds, initial_token, sample):\n",
    "    to_retrieve=seconds*2\n",
    "    sequence_tokens=[] #list of tokens in sequences\n",
    "\n",
    "    sequence_tokens.append(initial_token)\n",
    "    for i in range(0, to_retrieve):\n",
    "        following_token=sample['next']\n",
    "        sequence_tokens.append(following_token)\n",
    "        sample = nuscenes.get('sample', following_token)\n",
    "    return sequence_tokens\n",
    "\n",
    "#computes and overlaps all the bboxes\n",
    "#list of tokens and info complete as from the createdimages_original\n",
    "def getbboxes(tokens, info):\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216fd4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samp2ego(samp, nusc):\n",
    "    egopose = nusc.get('ego_pose', nusc.get('sample_data',\n",
    "                                            samp['data']['LIDAR_TOP'])\n",
    "                       ['ego_pose_token'])\n",
    "    rot = Quaternion(egopose['rotation']).rotation_matrix\n",
    "    rot = np.arctan2(rot[1, 0], rot[0, 0])\n",
    "    return {\n",
    "                'x': egopose['translation'][0],\n",
    "                'y': egopose['translation'][1],\n",
    "                'hcos': np.cos(rot),\n",
    "                'hsin': np.sin(rot),\n",
    "                'l': 4.084,\n",
    "                'w': 1.73,\n",
    "            }\n",
    "\n",
    "#example output:\n",
    "#samp2ego(nusc.get('sample', listtoken[1]), nusc)\n",
    "\n",
    "#{'x': 1663.7830538752974,\n",
    "# 'y': 1254.2029068912966,\n",
    "# 'hcos': 0.4754610435577135,\n",
    "# 'hsin': 0.8797367765752492,\n",
    "# 'l': 4.084, #ego dimensions\n",
    "# 'w': 1.73} #ego dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1c0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the sum of all ground truth positions in the considered tokens, with respect to ego\n",
    "def collect_x_gt(sample_tokens, #here put all the X tokens that we need to consider,\n",
    "                 #the token with ego is the first token\n",
    "              ego,            \n",
    "              nusc,\n",
    "              gt_boxes,\n",
    "              nusc_maps,\n",
    "              stretch=120,\n",
    "              layer_names=['road_segment', 'lane'],\n",
    "              line_names=['road_divider', 'lane_divider'],\n",
    "              nx=256, ny=256, \n",
    "              bx=np.array([-16.85, -38.35]),\n",
    "              dx=np.array([0.3, 0.3])):\n",
    "    \n",
    "    samp = nusc.get('sample', sample_tokens[0])\n",
    "    # local map\n",
    "    map_name = samp2mapname(samp, nusc)\n",
    "    lmap = get_local_map(nusc_maps[map_name],\n",
    "                             [ego['x'], ego['y'], ego['hcos'], ego['hsin']],\n",
    "                             stretch, layer_names, line_names)    \n",
    "    \n",
    "    gtlobjs=np.empty((10000,4))\n",
    "    gtlws=np.empty((10000, 2))\n",
    "    counter=0\n",
    "    for i in sample_tokens:\n",
    "        sample_token=i\n",
    "        samp = nusc.get('sample', sample_token)\n",
    "\n",
    "        # ground truths bboxes\n",
    "        gtlobjs_tmp, gtlws_tmp = get_other_objs(gt_boxes[sample_token],\n",
    "                                        np.array([ego['x'], ego['y'],\n",
    "                                                  ego['hcos'], ego['hsin']]))\n",
    "        \n",
    "        length=gtlobjs_tmp.shape[0]\n",
    "        \n",
    "        gtlobjs[counter:(counter+length)]=gtlobjs_tmp\n",
    "        gtlws[counter:(counter+length)]=gtlws_tmp\n",
    "        counter=counter+length\n",
    "\n",
    "    gtlobjs=gtlobjs[0:counter]\n",
    "    gtlws=gtlws[0:counter]\n",
    "    # render\n",
    "    gtx = raster_render(lmap, [ego['l'], ego['w']], gtlobjs, gtlws,\n",
    "                            nx, ny, layer_names, line_names,\n",
    "                            bx, dx)\n",
    "\n",
    "    return gtx\n",
    "\n",
    "#true if there are overlaps between the black area in a and b\n",
    "#very simple:black zones becomes equal to 10\n",
    "#if there is a 20, it means there is an overlap between images\n",
    "def compute_overlaps(a, b):\n",
    "    image=np.add(a, b)\n",
    "    if(np.max(np.unique(image))==2):\n",
    "        return True, image\n",
    "\n",
    "    return False, image\n",
    "\n",
    "\n",
    "#a = sum of ground truth\n",
    "#b = trajectory computed \n",
    "def save_results(a, b, token, string):\n",
    "    a[a==1]=0.5\n",
    "    image=np.add(a, b)#ground truth light grey, trajectory in black\n",
    "    image[image>1]=1\n",
    "    im = Image.fromarray(image)\n",
    "    if (im.mode != 'RGB'):\n",
    "        im = im.convert('RGB')\n",
    "    im.save(RESULTS_PATH+\"/\"+selected_detector+\"/\"+str(token)+\"_\"+str(string)+\".jpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ccc879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens is the list of \"seconds*2\" consecutive frames\n",
    "#(should be 8 frames from nuscenes)\n",
    "#computes overlaps in 1-seconds slots\n",
    "def do_computation(tokens,\n",
    "                   info_original,\n",
    "                   all_original,\n",
    "                   info_dynamic,\n",
    "                   all_dynamic,\n",
    "                   dx, bx, nx, ny, stretch, nusc, nusc_maps, gt_boxes\n",
    "                  ):\n",
    "    \n",
    "    #get ego\n",
    "    samp = nusc.get('sample', tokens[0])\n",
    "    ego = samp2ego(samp, nusc)\n",
    "\n",
    "    #get pkl score of the first token\n",
    "    pkl_measured_original=info_original['full'][tokens[0]]\n",
    "    pkl_measured_dynamic=info_dynamic['full'][tokens[0]]\n",
    "\n",
    "    \n",
    "    #get the ground truth trajectory\n",
    "    sGT=all_original[0]['heat_gt']\n",
    "    trajectory_pt=computetrajectory(sGT, number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    \n",
    "    #get trajectory original\n",
    "    sPT=all_original[0]['heat_pt']\n",
    "    trajectory_pt=computetrajectory(sPT, number_of_trajectory_poses, TRAJ_POINTS)\n",
    "\n",
    "    #get trajectory dynamic\n",
    "    sDT=all_dynamic[0]['heat_pt']\n",
    "    trajectory_pt=computetrajectory(sDT, number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    \n",
    "    #get the ground_truths bboxes of each token, with respect to ego, and overlap trajectories\n",
    "    for i in range(0, 6):#frames of nuscenes tokens list\n",
    "        j=2*i #the 16 frames of the trajectory planner\n",
    "        gt_with_respect_to_ego=collect_x_gt(tokens[i:i+3], #1 seconds\n",
    "                                            ego, nusc, gt_boxes, nusc_maps,\n",
    "                                            stretch=stretch, \n",
    "                                            layer_names=layer_names,\n",
    "                                            line_names=line_names,\n",
    "                                            nx=nx, ny=ny, bx=bx, dx=dx)[3]\n",
    "    \n",
    "        #gt trajectory and gt bboxes\n",
    "        trajectory_next_second=computetrajectory_min_max(sGT, minimum=j,\n",
    "                                                         maximum=j+5, #2 seconds\n",
    "                                                         number_of_trajectory_poses=16,\n",
    "                                                         TRAJ_POINTS=25)\n",
    "\n",
    "        overlap_gt, resulting_image=compute_overlaps(gt_with_respect_to_ego.copy(),\n",
    "                                                        trajectory_next_second.copy())\n",
    "    \n",
    "        if(overlap_gt==True):\n",
    "            print(\"overlap is \"+str(overlap_gt)+\" for token \"+str(tokens[0])+ \" at iteration i=\"+str(i))\n",
    "            save_results(gt_with_respect_to_ego.copy(), trajectory_next_second.copy(), token, \"GT BBoxes and GT trajectory\")\n",
    "\n",
    "        printfigure3x1(font, overlap_gt, gt_with_respect_to_ego,trajectory_next_second, resulting_image)\n",
    "\n",
    "        #original trajectory and gt bboxes\n",
    "        trajectory_next_second=computetrajectory_min_max(sPT, minimum=j,\n",
    "                                                         maximum=j+5, #2 seconds\n",
    "                                                         number_of_trajectory_poses=16,\n",
    "                                                         TRAJ_POINTS=25)\n",
    "\n",
    "        overlap_pt, resulting_image=compute_overlaps(gt_with_respect_to_ego.copy(),\n",
    "                                                        trajectory_next_second.copy())\n",
    "    \n",
    "        if(overlap_pt==True):\n",
    "            print(\"overlap is \"+str(overlap_pt)+\" for token \"+str(tokens[0])+ \" at iteration i=\"+str(i))\n",
    "            save_results(gt_with_respect_to_ego.copy(), trajectory_next_second.copy(), token, \"GT BBoxes and original PT trajectory\")\n",
    "\n",
    "        printfigure3x1(font, overlap_pt, gt_with_respect_to_ego,trajectory_next_second, resulting_image)\n",
    "\n",
    "            \n",
    "        #dynamic trajectory and gt bboxes\n",
    "        trajectory_next_second=computetrajectory_min_max(sDT, minimum=j,\n",
    "                                                         maximum=j+5, #2 seconds\n",
    "                                                         number_of_trajectory_poses=16,\n",
    "                                                         TRAJ_POINTS=25)\n",
    "\n",
    "        overlap_dt, resulting_image=compute_overlaps(gt_with_respect_to_ego.copy(),\n",
    "                                                        trajectory_next_second.copy())\n",
    "    \n",
    "        if(overlap_dt==True):\n",
    "            print(\"overlap is \"+str(overlap_pt)+\" for token \"+str(tokens[0])+ \" at iteration i=\"+str(i))\n",
    "            save_results(gt_with_respect_to_ego.copy(), trajectory_next_second.copy(), token, \"GT BBoxes and dynamic DT trajectory\")\n",
    "\n",
    "        printfigure3x1(font, overlap_dt, gt_with_respect_to_ego,trajectory_next_second, resulting_image)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7e750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot(h):\n",
    "    return np.array([\n",
    "        [np.cos(h), np.sin(h)],\n",
    "        [-np.sin(h), np.cos(h)],\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc672eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objects2frame(history, center, toworld=False):\n",
    "    \"\"\"A sphagetti function that converts from global\n",
    "    coordinates to \"center\" coordinates or the inverse.\n",
    "    It has no for loops but works on batchs.\n",
    "    \"\"\"\n",
    "    N, A, B = history.shape\n",
    "    theta = np.arctan2(center[3], center[2])\n",
    "    if not toworld:\n",
    "        newloc = history[:, :, :2] - center[:2].reshape((1, 1, 2))\n",
    "        rot = get_rot(theta).T\n",
    "        newh = np.arctan2(history[:, :, 3], history[:, :, 2]) - theta\n",
    "        newloc = np.dot(newloc.reshape((N*A, 2)), rot).reshape((N, A, 2))\n",
    "    else:\n",
    "        rot = get_rot(theta)\n",
    "        newh = np.arctan2(history[:, :, 3], history[:, :, 2]) + theta\n",
    "        newloc = np.dot(history[:, :, :2].reshape((N*A, 2)),\n",
    "                        rot).reshape((N, A, 2))\n",
    "    newh = np.stack((np.cos(newh), np.sin(newh)), 2)\n",
    "    if toworld:\n",
    "        newloc += center[:2]\n",
    "    return np.append(newloc, newh, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efad2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_render(lmap, centerlw, lobjs, lws,\n",
    "                  nx, ny, layer_names, line_names,\n",
    "                  bx, dx):\n",
    "    # draw both road layers vin one channel\n",
    "    road_img = np.zeros((nx, ny))\n",
    "    for layer_name in layer_names:\n",
    "        for poly in lmap[layer_name]:\n",
    "            # draw the lines\n",
    "            pts = np.round(\n",
    "                (poly - bx[:2] + dx[:2]/2.) / dx[:2]\n",
    "            ).astype(np.int32)\n",
    "            pts[:, [1, 0]] = pts[:, [0, 1]]\n",
    "            cv2.fillPoly(road_img, [pts], 1.0)\n",
    "\n",
    "    def draw_lane(layer_name, img):\n",
    "        for poly in lmap[layer_name]:\n",
    "            pts = np.round(\n",
    "                (poly - bx[:2] + dx[:2]/2.) / dx[:2]\n",
    "            ).astype(np.int32)\n",
    "            pts[:, [1, 0]] = pts[:, [0, 1]]\n",
    "            cv2.polylines(img, [pts], isClosed=False, color=1.0)\n",
    "        return img\n",
    "    road_div_img = np.zeros((nx, ny))\n",
    "    draw_lane('road_divider', road_div_img)\n",
    "    lane_div_img = np.zeros((nx, ny))\n",
    "    draw_lane('lane_divider', lane_div_img)\n",
    "\n",
    "    obj_img = np.zeros((nx, ny))\n",
    "    for box, lw in zip(lobjs, lws):\n",
    "        pts = get_corners(box, lw)\n",
    "        # draw the box\n",
    "        pts = np.round(\n",
    "            (pts - bx[:2] + dx[:2]/2.) / dx[:2]\n",
    "        ).astype(np.int32)\n",
    "        pts[:, [1, 0]] = pts[:, [0, 1]]\n",
    "        cv2.fillPoly(obj_img, [pts], 1.0)\n",
    "\n",
    "    center_img = np.zeros((nx, ny))\n",
    "    pts = get_corners([0.0, 0.0, 1.0, 0.0], centerlw)\n",
    "    pts = np.round(\n",
    "        (pts - bx[:2] + dx[:2]/2.) / dx[:2]\n",
    "    ).astype(np.int32)\n",
    "    pts[:, [1, 0]] = pts[:, [0, 1]]\n",
    "    cv2.fillPoly(center_img, [pts], 1.0)\n",
    "\n",
    "    return np.stack([road_img, road_div_img, lane_div_img,\n",
    "                     obj_img, center_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c647938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_objs(boxes, ego):\n",
    "    objs = []\n",
    "    lws = []\n",
    "    for box in boxes:\n",
    "        rot = Quaternion(box.rotation).rotation_matrix\n",
    "        rot = np.arctan2(rot[1, 0], rot[0, 0])\n",
    "\n",
    "        lws.append([box.size[1], box.size[0]])\n",
    "        objs.append([box.translation[0], box.translation[1],\n",
    "                     np.cos(rot), np.sin(rot)])\n",
    "    objs = np.array(objs)\n",
    "    lws = np.array(lws)\n",
    "    if len(objs) == 0:\n",
    "        lobjs = np.zeros((0, 4))\n",
    "    else:\n",
    "        lobjs = objects2frame(objs[np.newaxis, :, :],\n",
    "                              ego,\n",
    "                              )[0]\n",
    "    return lobjs, lws\n",
    "\n",
    "#lws is the dimension of the bbox\n",
    "#lobjs seems translation with respect to ego + rotation with respect to ego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de78ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corners(box, lw):\n",
    "    l, w = lw\n",
    "    simple_box = np.array([\n",
    "        [-l/2., -w/2.],\n",
    "        [l/2., -w/2.],\n",
    "        [l/2., w/2.],\n",
    "        [-l/2., w/2.],\n",
    "    ])\n",
    "    h = np.arctan2(box[3], box[2])\n",
    "    rot = get_rot(h)\n",
    "    simple_box = np.dot(simple_box, rot)\n",
    "    simple_box += box[:2]\n",
    "    return simple_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cb51123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(detector_file=\"none\",\n",
    "              val='val',\n",
    "              model='none',\n",
    "              d=1,\n",
    "              r=1,\n",
    "              t=1,\n",
    "              verbose=False,\n",
    "              recall_type=\"PRED AL NUMERATORE\",\n",
    "              nworkers=10, #for pkl\n",
    "              bsz= 128, #for pkl\n",
    "              gpuid=0# GPU ID, -1 if CPU only\n",
    "             ):\n",
    "\n",
    "    \n",
    "    dt=dcl.DetectionEval(nusc=nuscenes,\n",
    "        config=confvalue,\n",
    "        result_path=detector_file,\n",
    "        eval_set=val,\n",
    "        model_name=model,\n",
    "        MAX_DISTANCE_OBJ=d,\n",
    "        MAX_DISTANCE_INTERSECT=r,\n",
    "        MAX_TIME_INTERSECT_OBJ=t,\n",
    "        verbose=verbose,\n",
    "        recall_type=\"PRED AL NUMERATORE\",\n",
    "        nworkers=nworkers,\n",
    "        bsz=bsz,\n",
    "        gpuid=gpuid,output_dir='/home/notebook/pkl/results/GOAL3/remove/'\n",
    "        )\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b95ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crit_pkl(dt,\n",
    "                     listtoken=[],\n",
    "                     conf_th_list=[0.4],\n",
    "                     dist_list=[2.0],\n",
    "                     crit_list=[0.5],\n",
    "                     object_classes='car',\n",
    "                     verbose=False):\n",
    "\n",
    "    results= dt.safety_metric_evaluation(\n",
    "                    list_of_tokens=listtoken,\n",
    "                    conf_th_list=conf_th_list,\n",
    "                    dist_list=dist,\n",
    "                    crit_list=criticalities,\n",
    "                    obj_classes_list=object_classes, \n",
    "                    render_images=False,\n",
    "                    verbose=verbose\n",
    "                    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4df8279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes if the target configuration leads to an improvement of pkl in some way\n",
    "#fill results in a file containing:\n",
    "#DETECTOR;\n",
    "# pkl_dynamic['mean'], pkl_dynamic['median'], pkl_original['min']\n",
    "#pkl_original['mean'],pkl_original['median'], pkl_original['max']\n",
    "#pkl_GOAL1['mean'],pkl_GOAL1['median'], pkl_GOAL1['max']\n",
    "#d, r, t\n",
    "#criticality threshold for the dynamic\n",
    "#confidence threshold for the dynamic\n",
    "#threshold function for the dynamic\n",
    "def compute_improvement(pkl_dynamic, pkl_original,pkl_GOAL1, detector, d, r, t, \\\n",
    "                        function_name, criticality, confidence):\n",
    "    improvement=False\n",
    "    mean_pkl_improvement=pkl_GOAL1['mean']-pkl_dynamic_results['mean']\n",
    "    median_pkl_improvement=pkl_GOAL1['median']-pkl_dynamic_results['median']\n",
    "    maximum_pkl_improvement=pkl_GOAL1['max']-pkl_dynamic_results['max']\n",
    "\n",
    "    if(maximum_pkl_improvement>0 or mean_pkl_improvement > 0 or median_pkl_improvement> 0):\n",
    "        improvement=True\n",
    "        results={\"detector\": detector,\n",
    "                \"improvement\":improvement,\n",
    "                \"function\":str(function_name.__name__),\n",
    "                \"criticality\":criticality,\n",
    "                \"confidence\":confidence,\n",
    "                \"D, R, T\":[d, r, t],\n",
    "                \"mean_pkl_improvement\":mean_pkl_improvement,\n",
    "                \"median_pkl_improvement\":median_pkl_improvement,\n",
    "                \"max_pkl_improvement\":maximum_pkl_improvement,\n",
    "                \"mean_pkl_dynamic\":pkl_dynamic['mean'],\n",
    "                \"median_pkl_dynamic\":pkl_dynamic['median'],\n",
    "                \"max_pkl_dynamic\":pkl_dynamic['max'],\n",
    "                \"mean_pkl_GOAL1\":pkl_GOAL1['mean'],\n",
    "                \"median_pkl_GOAL1\":pkl_GOAL1['median'],\n",
    "                \"max_pkl_GOAL1\":pkl_GOAL1['max'],\n",
    "                \"mean_pkl\":pkl_original['mean'],\n",
    "                \"median_pkl\":pkl_original['median'],\n",
    "                \"max_pkl\":pkl_original['max'],\n",
    "           }\n",
    "\n",
    "        #save results: first time, file does not exists\n",
    "        if(os.path.isfile(RESULTS_PATH+detector+'/configurations_that_improve_pkl.json')==False):\n",
    "            with open(RESULTS_PATH+detector+'/configurations_that_improve_pkl.json','w') as f:\n",
    "                json.dump({detector: [results]}, f) \n",
    "        else: #file exists\n",
    "            with open(RESULTS_PATH+detector+'/configurations_that_improve_pkl.json','r') as f:\n",
    "                content=json.load(f) \n",
    "            with open(RESULTS_PATH+detector+'/configurations_that_improve_pkl.json','w+') as f:\n",
    "                list_result=content[detector] #get the list of \"results\" stored in the file\n",
    "                list_result.append(results)\n",
    "                json.dump({detector:list_result}, f)\n",
    "\n",
    "\n",
    "'''\n",
    "def compute_improvement(pkl_dynamic, pkl_original, detector, d, r, t, \\\n",
    "                        function_name, criticality, confidence):\n",
    "    improvement=False\n",
    "    if(len(pkl_original)==1):\n",
    "                      #pkl for the mean, median and max values\n",
    "                      #computed using the best confidence threshold\n",
    "                      #which is always the same, in this case\n",
    "        pkl_original=[pkl_original[0],pkl_original[0],pkl_original[0]]\n",
    "        \n",
    "    mean_pkl_improvement=pkl_original[0]['mean']-pkl_dynamic_results['mean']\n",
    "    median_pkl_improvement=pkl_original[1]['median']-pkl_dynamic_results['median']\n",
    "    maximum_pkl_improvement=pkl_original[2]['max']-pkl_dynamic_results['max']\n",
    "\n",
    "    if(maximum_pkl_improvement>0 or mean_pkl_improvement > 0 or median_pkl_improvement> 0):\n",
    "        improvement=True\n",
    "        results={\"detector\": detector,\n",
    "                \"improvement\":improvement,\n",
    "                \"function\":str(function_name.__name__),\n",
    "                \"criticality\":criticality,\n",
    "                \"confidence\":confidence,\n",
    "                \"D, R, T\":[d, r, t],\n",
    "                \"mean_pkl_improvement\":mean_pkl_improvement,\n",
    "                \"median_pkl_improvement\":median_pkl_improvement,\n",
    "                \"max_pkl_improvement\":maximum_pkl_improvement,\n",
    "                \"min_pkl_dynamic\":pkl_dynamic['min'],\n",
    "                \"mean_pkl_dynamic\":pkl_dynamic['mean'],\n",
    "                \"median_pkl_dynamic\":pkl_dynamic['median'],\n",
    "                \"max_pkl_dynamic\":pkl_dynamic['max'],\n",
    "                \"min_pkl_original\":pkl_original[0]['min'],\n",
    "                \"mean_pkl_original\":pkl_original[0]['mean'],\n",
    "                \"median_pkl_original\":pkl_original[1]['median'],\n",
    "                \"max_pkl_original\":pkl_original[2]['max'],\n",
    "           }\n",
    "\n",
    "        #save results: first time, file does not exists\n",
    "        if(os.path.isfile(RESULTS_PATH+detector+'/configurations_that_improve_pkl.json')==False):\n",
    "            with open(RESULTS_PATH+detector+'/configurations_that_improve_pkl.json','w') as f:\n",
    "                json.dump({detector: [results]}, f) \n",
    "        else: #file exists\n",
    "            with open(RESULTS_PATH+detector+'/configurations_that_improve_pkl.json','r') as f:\n",
    "                content=json.load(f) \n",
    "            with open(RESULTS_PATH+detector+'/configurations_that_improve_pkl.json','w+') as f:\n",
    "                list_result=content[detector] #get the list of \"results\" stored in the file\n",
    "                list_result.append(results)\n",
    "                json.dump({detector:list_result}, f)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37cf01",
   "metadata": {},
   "source": [
    "**Configuration items**\n",
    "\n",
    "Configuration parameters. Rather intuitive: \n",
    "\n",
    "* set paths\n",
    "\n",
    "* for all the rest, leave unaltered what you do not understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ece0241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL=\"GOAL3\" #set GOAL3, no alternative\n",
    "car_only=False #filters out all non-car objects\n",
    "\n",
    "\n",
    "verbose=False #strongly recommended \"False\"\n",
    "nworkers=10 # number of workers for dataloader: depends on your resources!\n",
    "bsz= 128# batch size for dataloader: depends on your resources!\n",
    "gpuid=0 #id of the gpu; -1 if no GPU\n",
    "\n",
    "DETECTOR= {\n",
    "#           \"SECFPN\": 'SECFPN',\n",
    "#           \"FCOS3D\": 'FCOS3D',\n",
    "#           \"PGD\": 'PGD',\n",
    "#           \"SSN\": 'SSN',\n",
    "           \"PointPillars\": 'POINTP',\n",
    "#           \"RegNetX-1.6gF-FPN\":'REG',\n",
    "        }\n",
    "\n",
    "#these below depends on the detector, it comes from GOAL 1\n",
    "#depending on the detector, we have different thresholds that maximise pkl        \n",
    "#and different d, r, t\n",
    "#and different criticality scores\n",
    "#same cases for both car_only==True and False\n",
    "def set_threshold(detector):\n",
    "    if(detector=='FCOS3D'):\n",
    "        conf_list=list(np.arange(0.05, 0.35, 0.1))\n",
    "    elif(detector=='PGD'):\n",
    "        conf_list=list(np.arange(0.05, 0.35, 0.1))\n",
    "    elif(detector=='POINTP'):\n",
    "        conf_list=list(np.arange(0.5, 0.75, 0.05))\n",
    "    elif(detector=='REG'):\n",
    "        conf_list=list(np.arange(0.4, 0.7, 0.1))\n",
    "    elif(detector=='SECFPN'):\n",
    "        conf_list=list(np.arange(0.4, 0.7, 0.1))\n",
    "    elif(detector=='SSN'):\n",
    "        conf_list=list(np.arange(0.2, 0.5, 0.1))\n",
    "    return conf_list\n",
    "\n",
    "def set_d_r_t():\n",
    "    MAX_D=list(range(20, 45, 5))\n",
    "    MAX_R=list(range(5, 20, 5))\n",
    "    MAX_T=list(range(4, 12, 4))\n",
    "    #distance_intersect_time combinations\n",
    "    DRT = list(itertools.product(*[MAX_D, MAX_R, MAX_T]))\n",
    "    return DRT\n",
    "\n",
    "#it is the optimal threshold for our detector; a unique value\n",
    "#we retrieve experimentally\n",
    "def get_optimal_threshold(detector):\n",
    "    if(car_only==False):\n",
    "        if(detector=='FCOS3D'):\n",
    "            return [0.15, 0.25, 0.25]\n",
    "        elif(detector=='PGD'):\n",
    "            return [0.05]\n",
    "        elif(detector=='POINTP'):\n",
    "            return [0.55]\n",
    "        elif(detector=='REG'):\n",
    "            return [0.55]\n",
    "        elif(detector=='SECFPN'):\n",
    "            return [0.50]\n",
    "        elif(detector=='SSN'):\n",
    "            return [0.25, 0.30, 0.30]\n",
    "        \n",
    "    if(car_only==True):\n",
    "        print(\"get_optimal_threshold car_only TODO\")\n",
    "        return \" get_optimal_PKL car_only TODO\"\n",
    "    return \"error\"\n",
    "\n",
    "def get_optimal_PKL(detector):\n",
    "    if(car_only==False):\n",
    "        if(detector=='FCOS3D'):\n",
    "            combo= [4.104, 0.986, 98.922]\n",
    "        elif(detector=='PGD'):\n",
    "            combo= [4.778, 1.029, 116,456]\n",
    "        elif(detector=='POINTP'):\n",
    "            combo= [78.328, 24.857, 379.112]\n",
    "        elif(detector=='REG'):\n",
    "            combo= [78.951, 22.749, 380.456]\n",
    "        elif(detector=='SECFPN'):\n",
    "            combo= [65.561, 16.495, 390.986]\n",
    "        elif(detector=='SSN'):\n",
    "            combo= [3.490, 0.706, 118.750]\n",
    "        \n",
    "        return {\"mean\": combo[0], \"median\": combo[1], \"max\": combo[2]}\n",
    "        \n",
    "    if(car_only==True):\n",
    "        print(\" get_optimal_PKL car_only TODO\")\n",
    "        return \" get_optimal_PKL car_only TODO\"\n",
    "\n",
    "def get_optimal_PKLGOAL1(detector):\n",
    "    if(car_only==False):\n",
    "        if(detector=='FCOS3D'):\n",
    "            combo= [4.080,0.966, 98.922]\n",
    "        elif(detector=='PGD'):\n",
    "            combo= [4.741,0.973, 116.456]\n",
    "        elif(detector=='POINTP'):\n",
    "            combo= [45.046,8.985, 381.499 ]\n",
    "        elif(detector=='REG'):\n",
    "            combo= [43.801, 9.044, 378.434]\n",
    "        elif(detector=='SECFPN'):\n",
    "            combo= [38.284,8.378, 376.604 ]\n",
    "        elif(detector=='SSN'):\n",
    "            combo= [3.709, 0.719, 118.750]\n",
    "        \n",
    "        return {\"mean\": combo[0], \"median\": combo[1], \"max\": combo[2]}\n",
    "\n",
    "    if(car_only==True):\n",
    "        print(\"get_optimal_PKLGOAL1 car_only TODO\")\n",
    "        return \"get_optimal_PKLGOAL1 car_only TODO\"\n",
    "    return \"error\"\n",
    "        \n",
    "def criticality_scores():\n",
    "    return list(np.arange(0.9, 0.99, 0.05))\n",
    "\n",
    "\n",
    "HOME='/home/notebook/'\n",
    "#nuscene dataset install folder\n",
    "DATAROOT = HOME+'nuscene/data/'\n",
    "\n",
    "#pkl path to planner.pt and masks_trainval.json\n",
    "modelpath=HOME+'/pkl/Evaluation-Of-Safety-Oriented-Metrics-for-Object-Detectors/metrics_model/planner.pt'\n",
    "mask_json=HOME+'pkl/Evaluation-Of-Safety-Oriented-Metrics-for-Object-Detectors/metrics_model/masks_trainval.json'\n",
    "\n",
    "#results of the object detectors (result_nusc.json) are stored here, in subdirectories:\n",
    "#PATH+DETECTOR+FILE_JSON\n",
    "#e.g. '/home/notebook/pkl/result_objdet/PGD/results_nusc.json'\n",
    "PATH=HOME+'/pkl/result_objdet/'\n",
    "FILE_JSON='/results_nusc.json'\n",
    "\n",
    "#results computed are stored here\n",
    "RESULTS_PATH=HOME+'/pkl/results/GOAL3/'\n",
    "\n",
    "#the classes we consider. We can create \"set of objects\". It accepts 2 sets of objects.\n",
    "#however we have a criticality model only for cars -- so do not remove cars from the list\n",
    "#TODO WE NEED CURRENTLY TO EXECUTE EVERYTHING TWICE; THE object_classes_reduced does nothing\n",
    "object_classes=['car', 'truck', 'bus', 'trailer', 'construction_vehicle', 'pedestrian', 'motorcycle', 'bicycle', 'traffic_cone', 'barrier']    \n",
    "object_classes_reduced=['car']\n",
    "\n",
    "if car_only:\n",
    "    object_classes=object_classes_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89a3dcbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 28.018 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 7.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "nuscenes = NuScenes('v1.0-trainval', dataroot=DATAROOT)\n",
    "confvalue=cnfig.config_factory(\"detection_cvpr_2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1685adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of scene that compose the val set\n",
    "eval=val = ['scene-0013',\n",
    "                'scene-0554',\n",
    "                'scene-0771',\n",
    "                'scene-0929',\n",
    "                'scene-1070',\n",
    "                'scene-1072',\n",
    "                'scene-0798',\n",
    "                'scene-0108',\n",
    "                'scene-0519',\n",
    "                'scene-0332',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e35a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of tokens in the val set (150 scenes), put it in a file to be used as input\n",
    "scenes_list=[]\n",
    "counter=0\n",
    "\n",
    "for i in nuscenes.scene:\n",
    "    name=i['name']\n",
    "    if(name in val):\n",
    "        counter=counter+1\n",
    "        scenes_list.append(i)\n",
    "\n",
    "validation_samples={}\n",
    "for i in scenes_list:\n",
    "    scene_name=i['name']\n",
    "    sample_token_list=[]\n",
    "    first_sample_token=i['first_sample_token']\n",
    "    last_sample_token=i['last_sample_token']\n",
    "    current_sample_token=first_sample_token\n",
    "    sample_token_list.append(current_sample_token)\n",
    "    if(sample_token_list[0]!=first_sample_token):\n",
    "        print(\"error\")\n",
    "        break\n",
    "    while(current_sample_token!=last_sample_token):\n",
    "        sample=nuscenes.get('sample', current_sample_token)\n",
    "        current_sample_token=sample['next']\n",
    "        sample_token_list.append(current_sample_token)\n",
    "    if(sample_token_list[len(sample_token_list)-1]!=last_sample_token):\n",
    "        print(\"error\")\n",
    "        break\n",
    "    \n",
    "    validation_samples.update({scene_name:sample_token_list})\n",
    "\n",
    "listtoken=[]\n",
    "for i in validation_samples.keys():\n",
    "    listtoken.extend(validation_samples[i])\n",
    "\n",
    "alltoken=listtoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9483a",
   "metadata": {},
   "source": [
    "**Here we define the functions to define the thresholds in a dynamic way**\n",
    "\n",
    "Currently we have just a simple condition:\n",
    "\n",
    "- *if confidence_score > basic_threshold * corr_factor_score*  --> keep bbox\n",
    "\n",
    "- *if criticality > basic_criticality and confidence_score < basic_threshold /corr_factor_score* --> keep bbox\n",
    "\n",
    "\n",
    "To add more alternatives, just add a new function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "530ff0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRT=set_d_r_t()#D,R,T values to try\n",
    "criticality_th=criticality_scores() #criticality thresholds to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8a57f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can add here more function to operate as correction factor    \n",
    "def create_list_corr_factors(detector, conf_th_list):\n",
    "    \n",
    "    correction_factor1_parameters=list(itertools.product(*[criticality_th, conf_th_list]))\n",
    "\n",
    "    return [{\"correction_function\": correction_factor1_25, \"parameters\":correction_factor1_parameters},\n",
    "            {\"correction_function\": correction_factor1_2, \"parameters\":correction_factor1_parameters},\n",
    "            {\"correction_function\": correction_factor1_3, \"parameters\":correction_factor1_parameters},\n",
    "            {\"correction_function\": correction_factor1_35, \"parameters\":correction_factor1_parameters},\n",
    "            # {\"correction_function\": correction_factor1_1, \"parameters\":correction_factor1_parameters} ,\n",
    "           # {\"correction_function\": correction_factor0_9, \"parameters\":correction_factor1_parameters} \n",
    "           ]\n",
    "\n",
    "\n",
    "#here is the first function we try\n",
    "#each function has a different correction_factor\n",
    "def correction_factor1_5(box_criticality, box_detection_score, criticality_th, conf_th):\n",
    "    corr_factor_score=1.5\n",
    "    if(box_detection_score > conf_th*corr_factor_score):\n",
    "        return True\n",
    "    elif(box_detection_score > conf_th/corr_factor_score and box_criticality> criticality_th ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def correction_factor1_25(box_criticality, box_detection_score, criticality_th, conf_th):\n",
    "    corr_factor_score=1.25\n",
    "    if(box_detection_score > conf_th*corr_factor_score):\n",
    "        return True\n",
    "    elif(box_detection_score > conf_th/corr_factor_score and box_criticality> criticality_th ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def correction_factor1_2(box_criticality, box_detection_score, criticality_th, conf_th):\n",
    "    corr_factor_score=1.2\n",
    "    if(box_detection_score > conf_th*corr_factor_score):\n",
    "        return True\n",
    "    elif(box_detection_score > conf_th/corr_factor_score and box_criticality> criticality_th ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def correction_factor1_3(box_criticality, box_detection_score, criticality_th, conf_th):\n",
    "    corr_factor_score=1.3\n",
    "    if(box_detection_score > conf_th*corr_factor_score):\n",
    "        return True\n",
    "    elif(box_detection_score > conf_th/corr_factor_score and box_criticality> criticality_th ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def correction_factor1_35(box_criticality, box_detection_score, criticality_th, conf_th):\n",
    "    corr_factor_score=1.35\n",
    "    if(box_detection_score > conf_th*corr_factor_score):\n",
    "        return True\n",
    "    elif(box_detection_score > conf_th/corr_factor_score and box_criticality> criticality_th ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def correction_factor1_1(box_criticality, box_detection_score, criticality_th, conf_th):\n",
    "    corr_factor_score=1.1\n",
    "    if(box_detection_score > conf_th*corr_factor_score):\n",
    "        return True\n",
    "    elif(box_detection_score > conf_th/corr_factor_score and box_criticality> criticality_th ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def correction_factor0_85(box_criticality, box_detection_score, criticality_th, conf_th):\n",
    "    corr_factor_score=0.85\n",
    "    if(box_detection_score > conf_th*corr_factor_score):\n",
    "        return True\n",
    "    elif(box_detection_score > conf_th/corr_factor_score and box_criticality> criticality_th ):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d8a8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector in DETECTOR:\n",
    "    #create results folder\n",
    "    if not os.path.exists(RESULTS_PATH+DETECTOR[detector]+\"/\"): \n",
    "        print(\"creating results folder {}\".format(RESULTS_PATH+DETECTOR[detector]+\"/\"))\n",
    "        os.makedirs(RESULTS_PATH+DETECTOR[detector], exist_ok = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e34b6",
   "metadata": {},
   "source": [
    "Iterate on all detector to find if there are some nice configurations that we may want to explore later on, i.e., some cases where using our approach pkl is reduced either in mean, median or max pkl.\n",
    "\n",
    "Results are saved in the GOAL3 folders that are created.\n",
    "\n",
    "Each value is computed against the optimal pkl setting either for mean, median and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create dt for PointPillars 20 5 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function correction_factor1_25, criticality 0.9, confidence 0.5, drt (20 5 4) \n",
      "loading pkl model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notebook/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/notebook/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 76.8910903930664, median 25.68929672241211, max 381.71044921875]\n",
      "function correction_factor1_25, criticality 0.9, confidence 0.55, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 72.39448547363281, median 23.092151641845703, max 386.60845947265625]\n",
      "function correction_factor1_25, criticality 0.9, confidence 0.6000000000000001, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 66.89276885986328, median 22.25596809387207, max 379.06182861328125]\n",
      "function correction_factor1_25, criticality 0.9, confidence 0.6500000000000001, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 64.39440155029297, median 22.408906936645508, max 384.04364013671875]\n",
      "function correction_factor1_25, criticality 0.9, confidence 0.7000000000000002, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 53.37868881225586, median 23.4782772064209, max 381.4987487792969]\n",
      "function correction_factor1_25, criticality 0.9500000000000001, confidence 0.5, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 76.62406158447266, median 26.60997772216797, max 379.27911376953125]\n",
      "function correction_factor1_25, criticality 0.9500000000000001, confidence 0.55, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 72.71944427490234, median 23.231048583984375, max 385.45361328125]\n",
      "function correction_factor1_25, criticality 0.9500000000000001, confidence 0.6000000000000001, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 67.4433364868164, median 22.311641693115234, max 378.0516357421875]\n",
      "function correction_factor1_25, criticality 0.9500000000000001, confidence 0.6500000000000001, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 64.17711639404297, median 22.353792190551758, max 381.600341796875]\n",
      "function correction_factor1_25, criticality 0.9500000000000001, confidence 0.7000000000000002, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 52.8126220703125, median 23.29442024230957, max 376.2462158203125]\n",
      "function correction_factor1_2, criticality 0.9, confidence 0.5, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 77.64677429199219, median 25.378477096557617, max 378.7628173828125]\n",
      "function correction_factor1_2, criticality 0.9, confidence 0.55, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 73.4712142944336, median 23.28973388671875, max 386.60845947265625]\n",
      "function correction_factor1_2, criticality 0.9, confidence 0.6000000000000001, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 69.58491516113281, median 22.311641693115234, max 378.524169921875]\n",
      "function correction_factor1_2, criticality 0.9, confidence 0.6500000000000001, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 65.71942138671875, median 22.401870727539062, max 380.6645812988281]\n",
      "function correction_factor1_2, criticality 0.9, confidence 0.7000000000000002, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 57.45894241333008, median 22.861194610595703, max 381.713623046875]\n",
      "function correction_factor1_2, criticality 0.9500000000000001, confidence 0.5, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 77.35869598388672, median 25.68929672241211, max 377.29071044921875]\n",
      "function correction_factor1_2, criticality 0.9500000000000001, confidence 0.55, drt (20 5 4) \n",
      "loading pkl model\n",
      "pkls computed with output of size : 403\n",
      "[pkl dynamic: mean 73.28765869140625, median 23.591299057006836, max 385.45361328125]\n",
      "function correction_factor1_2, criticality 0.9500000000000001, confidence 0.6000000000000001, drt (20 5 4) \n"
     ]
    }
   ],
   "source": [
    "for detector in DETECTOR:\n",
    "    conf_th_list=set_threshold(DETECTOR[detector])  #confidence thresholds to try\n",
    "\n",
    "    list_of_correction_factors=create_list_corr_factors(DETECTOR[detector], conf_th_list)\n",
    "\n",
    "    pkl_crit_results_store=[]\n",
    "    detector_file=PATH+DETECTOR[detector]+FILE_JSON\n",
    "    optimal_thresholds=get_optimal_threshold(DETECTOR[detector])\n",
    "    optimal_PKL=get_optimal_PKL(DETECTOR[detector])\n",
    "    optimal_PKLGOAL1=get_optimal_PKLGOAL1(DETECTOR[detector])\n",
    "        \n",
    "    for d, r, t in DRT:\n",
    "        print(\"create dt for {} {} {} {}\".format(detector, d, r, t))\n",
    "        dt=create_dt(detector_file,\n",
    "                         'val',\n",
    "                         model=detector_file,\n",
    "                         d=d,\n",
    "                         r=r,\n",
    "                         t=t,\n",
    "                         verbose=verbose,\n",
    "                         recall_type=\"PRED AL NUMERATORE\")\n",
    "        '''\n",
    "        #NO_correction_factor; just the plain pkl\n",
    "        if(len(optimal_thresholds)==1):\n",
    "            pkl_original=dt.calc_sample_crit_GOAL3(listtoken=alltoken,\n",
    "                                             conf_th=optimal_thresholds[0],\n",
    "                                             crit=5.0,#not considered as it is the \"normal\" pkl\n",
    "                                             correction_factor=None, #in this case, it is just the normal pkl\n",
    "                                             obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                             verbose=verbose)\n",
    "            pkl_original=[pkl_original]\n",
    "            print(\"[pkl original: mean {}, median {}, max {}]\".format(pkl_original[0]['mean'], pkl_original[0]['median'], pkl_original[0]['max']) )\n",
    "            \n",
    "        else:#len is 3, we have an optimal threshold that is different for mean, median and max values\n",
    "            pkl_original_mean_best=dt.calc_sample_crit_GOAL3(listtoken=alltoken,\n",
    "                                             conf_th=optimal_thresholds[0],#conf_th for the best mean pkl\n",
    "                                             crit=5.0,#not considered as it is the \"normal\" pkl\n",
    "                                             correction_factor=None, #in this case, it is just the normal pkl\n",
    "                                             obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                             verbose=verbose)\n",
    "            \n",
    "            pkl_original_median_best=dt.calc_sample_crit_GOAL3(listtoken=alltoken,\n",
    "                                             conf_th=optimal_thresholds[1],#conf_th for the best median pkl\n",
    "                                             crit=5.0,#not considered as it is the \"normal\" pkl\n",
    "                                             correction_factor=None, #in this case, it is just the normal pkl\n",
    "                                             obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                             verbose=verbose)\n",
    "\n",
    "            pkl_original_max_best=dt.calc_sample_crit_GOAL3(listtoken=alltoken,\n",
    "                                             conf_th=optimal_thresholds[2],#conf_th for the best max pkl\n",
    "                                             crit=5.0,#not considered as it is the \"normal\" pkl\n",
    "                                             correction_factor=None, #in this case, it is just the normal pkl\n",
    "                                             obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                             verbose=verbose)\n",
    "\n",
    "            pkl_original=[pkl_original_mean_best, pkl_original_median_best, pkl_original_max_best]\n",
    "            print(\"[pkl original: mean {}, median {}, max {}]\".format(pkl_original_mean_best[0]['mean'], pkl_original_median_best[0]['median'], pkl_original_max_best[0]['max']) )\n",
    "        ''';\n",
    "\n",
    "        for function in list_of_correction_factors:#function to compute the multiple threshold\n",
    "            parameters=function['parameters']\n",
    "            correction_function=function['correction_function']\n",
    "            for criticality, confidence in parameters:\n",
    "                print(\"function {}, criticality {}, confidence {}, drt ({} {} {}) \".format(str(function['correction_function'].__name__), criticality, confidence, d,r,t))\n",
    "                #correction function is now active\n",
    "                pkl_dynamic_results=dt.calc_sample_crit_GOAL3(listtoken=alltoken,\n",
    "                                                     conf_th=confidence,\n",
    "                                                     crit=criticality,\n",
    "                                                     correction_factor=correction_function,\n",
    "                                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                                     verbose=verbose)\n",
    "                \n",
    "                compute_improvement(pkl_dynamic_results, optimal_PKL,optimal_PKLGOAL1, DETECTOR[detector], d, r, t, correction_function, criticality,confidence)\n",
    "                print(\"[pkl dynamic: mean {}, median {}, max {}]\".format(pkl_dynamic_results['mean'], pkl_dynamic_results['median'],pkl_dynamic_results['max']) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07fc30f",
   "metadata": {},
   "source": [
    "Let's check if we have improvements, and what are the most relevant improvements. These are the nice configurations to maintain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"/home/notebook/pkl/results/GOAL3/\"\n",
    "FILENAME='configurations_that_improve_pkl.json'\n",
    "            \n",
    "for i in DETECTOR:\n",
    "    print(DETECTOR[i])\n",
    "    d1=get_optimal_PKLGOAL1(DETECTOR[i])\n",
    "    mean_GOAL1, median_GOAL1, max_GOAL1=d1['mean'], d1['median'], d1['max']\n",
    "    d2=get_optimal_PKL(DETECTOR[i])\n",
    "    mean_PKL, median_PKL, max_PKL=d2['mean'], d2['median'], d2['max']\n",
    "    \n",
    "    if(os.path.isfile(RESULTS_PATH+DETECTOR[i]+'/configurations_that_improve_pkl.json')==True):\n",
    "        with open(RESULTS_PATH+DETECTOR[i]+'/configurations_that_improve_pkl.json','r') as f:\n",
    "            data=json.load(f)\n",
    "    else:\n",
    "        print(\"file not found for {}\"+i)\n",
    "        continue\n",
    "\n",
    "    for d in data:\n",
    "        best_mean_data=None\n",
    "        best_median_data=None\n",
    "        best_max_data=None\n",
    "        mean_local=99999\n",
    "        median_local=9999\n",
    "        max_local=9999\n",
    "        for j in data[d]:\n",
    "            if(mean_local > j['mean_pkl_dynamic']):\n",
    "                mean_local=j['mean_pkl_dynamic'] \n",
    "                best_mean_data=j\n",
    "            if(median_local > j['median_pkl_dynamic']):\n",
    "                median_local = j['median_pkl_dynamic']\n",
    "                best_median_data=j\n",
    "            if(max_local >  j['max_pkl_dynamic']):\n",
    "                max_local=  j['max_pkl_dynamic']\n",
    "                best_max_data=j\n",
    "\n",
    "        if(mean_local<mean_GOAL1):\n",
    "            print(\"mean better than GOAL1 and OriginalPKL for {}\".format(i))\n",
    "            print(best_mean_data)\n",
    "        elif(mean_local<mean_PKL):\n",
    "            print(\"mean better than OriginalPKL for {}\".format(i))\n",
    "            print(best_mean_data)\n",
    "        if(median_local<median_GOAL1):\n",
    "            print(\"median better than GOAL1 and OriginalPKL for {}\".format(i))\n",
    "            print(best_median_data)\n",
    "        elif(median_local<median_PKL):\n",
    "            print(\"median better than OriginalPKL for {}\".format(i))\n",
    "            print(best_mean_data)\n",
    "        if(max_local<max_GOAL1):\n",
    "            print(\"max better than GOAL1 and OriginalPKL for {}\".format(i))\n",
    "            print(best_max_data)\n",
    "        elif(max_local<max_PKL):\n",
    "            print(\"max better than OriginalPKL for {}\".format(i))\n",
    "            print(best_max_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480073e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018ae3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f704f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d69b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7e85854",
   "metadata": {},
   "source": [
    "**The step below does not need a for loop, so it is removed. It is maintained in the other notebook, if needed to create some nice images, and can be used to investigate specific settings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816ba5e",
   "metadata": {},
   "source": [
    "We select most interesting frames, i.e., those we have most relevant improvement, and we show the corresponding pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X=len(pkl_dynamic_results['full']) #total number of frames to analyze\n",
    "X\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Convert dictionary to list\n",
    "\n",
    "#pkl_d is the list of tokens for dynamic pkl (our modified function)\n",
    "#pkl_o is the list of tokens for original pkl function\n",
    "pkl_d, pkl_o=pkl_dynamic_results['full'], pkl_original['full']\n",
    "list_pkl_d=[]\n",
    "list_pkl_o=[]\n",
    "\n",
    "for a in iter(pkl_d):\n",
    "    list_pkl_d.append([a, pkl_d[a]])\n",
    "\n",
    "for a in iter(pkl_o):\n",
    "    list_pkl_o.append([a, pkl_o[a]])\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''only_pkl_values=[]\n",
    "for i in list_pkl_d:\n",
    "    only_pkl_values.append(i[1])\n",
    "\n",
    "top_X_index=np.argsort(only_pkl_values)[-X:]\n",
    "top_X_index_pkl_d=itemgetter(*top_X_index)(list_pkl_d)\n",
    "\n",
    "only_pkl_values=[]\n",
    "for i in list_pkl_o:\n",
    "    only_pkl_values.append(i[1])\n",
    "\n",
    "top_X_index=np.argsort(only_pkl_values)[-X:]\n",
    "top_X_index_pkl_o=itemgetter(*top_X_index)(list_pkl_o)\n",
    "\n",
    "print(\"we have sorted the {} tokens based on pkl, for dynamic and original\".format(X))\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e4091",
   "metadata": {},
   "source": [
    "Select:\n",
    "    \n",
    "1- n frame with maximum difference between pkl with original function and our modified function (i.e., our modified function is closer to the ground truth)<br>\n",
    "2- k frame where original function has maximum pkl<br>\n",
    "3- k frame where dynamic function has maximum pkl<br>\n",
    "\n",
    "if the frames are the same in points 1, 2, 3, only one sample is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n=5\n",
    "k=5\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#k frame where original function has maximum pkl\n",
    "original_max=top_X_index_pkl_o[-k:]\n",
    "#k frame where dynamic function has maximum pkl\n",
    "dynamic_max=top_X_index_pkl_d[-k:]\n",
    "\n",
    "#n frame of pkl difference maximum between original and dynamic (dynamic is better)\n",
    "tmp=[]\n",
    "for i in range(0, len(list_pkl_d)):\n",
    "    value=list_pkl_o[i][1]-list_pkl_d[i][1]\n",
    "    if(value>0):\n",
    "        tmp.append([list_pkl_d[i][0], value])\n",
    "max_gain_d_over_o=sorted(tmp, key=operator.itemgetter(1))[-n:]\n",
    "\n",
    "\n",
    "#additional information\n",
    "np_pkl_o=np.asarray(list_pkl_o)[:,1].astype(float)\n",
    "np_pkl_d=np.asarray(list_pkl_d)[:,1].astype(float)\n",
    "difference=np_pkl_d-np_pkl_o\n",
    "\n",
    "print(\"number of values where dynamic pkls is equal than original is {}\".format(len(difference[difference==0])))\n",
    "\n",
    "#difference: dynamic smaller than original\n",
    "print(\"number of values where dynamic pkls are smaller than original is {}\".format(len(difference[difference<0])))\n",
    "\n",
    "#index sorted, from lowest (most different) to highest\"\n",
    "dynamic_smaller_index=np.argsort(difference)\n",
    "top_X_pkl_d_smaller=itemgetter(*dynamic_smaller_index[:X])(list_pkl_d)\n",
    "\n",
    "#difference: dynamic bigger than original\n",
    "print(\"number of values where dynamic pkls are larger than original is {}\".format(len(difference[difference>0])))\n",
    "\n",
    "#original_smaller_index=np.argsort(difference)\n",
    "#top_X_pkl_o_smaller=itemgetter(*original_smaller_index[-X:])(list_pkl_o)\n",
    "#print(\"indexes for dynamic smaller, and for original smaller are respectively in top_X_pkl_d_smaller, top_X_pkl_o_smaller \")\n",
    "#print(\"dynamic: {}\".format(top_X_pkl_d_smaller))\n",
    "#print(\"original: {}\".format(top_X_pkl_o_smaller)) \n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e9bd9",
   "metadata": {},
   "source": [
    "*and finally we visualize the results!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9991",
   "metadata": {},
   "source": [
    "We use a modified version of the pkl library and related functions, so that we can visualize the required information.\n",
    "We obtain the following variables. We report them here as they are a too many and confusing.\n",
    "\n",
    "- pklfile_original: only configuration parameters, e.g., the handler to the correction factor used\n",
    "- pklfile_dynamic:  only configuration parameters, e.g., the handler to the correction factor used\n",
    "\n",
    "\n",
    "- info_original: pkl min, max, mean, median computed on the target sample, using the traditional approach (no correction factor). If it is only 1 token, expect a unique value\n",
    "- info_dynamic : pkl min, max, mean, median  computed on the target sample, using the correction factor. If it is only 1 token, expect a unique value\n",
    "\n",
    "\n",
    "- all_pkls_original: pkl measures with no correction factor (original approach) \n",
    "- all_pkls_dynamic: pkl measures with correction factor \\\n",
    "\n",
    "\n",
    "- gtdist: ground truth trajectories. It is an array of shape [x, 16, 256, 256]. For each of the X images, there are 16 frames of shape [256, 256] frames: 16 planned trajectories through time, from t+0.25 until t+4.0 seconds. In other words, it is the forecast at time {t + 0.25k | 1  k  16}. The trajectory is plotted as \"most likely trajectory\" on a shape of [256, 256].\n",
    "- preddist_original: like gtdist, but for trajectories planned using the predicted bbox (and original threshold)\n",
    "\n",
    "- preddist_dynamic:  like gtdist, but for trajectories planned using the predicted bbox with the correction factor\n",
    "\n",
    "\n",
    "- gtxs: array of shape [x, 5, 256, 256], where for each image X, it has:\n",
    "    - position 0: the road\n",
    "    - position 1: road separators, if any\n",
    "    - position 2: lane separators, if any\n",
    "    - position 3: ground truth position of objects\n",
    "    - position 4: position of ego\n",
    "    \n",
    "- predxs_original: same as gtxs, but for prediction with no correction factor (original approach). The difference with respect to gtxs is only on position of objects.\n",
    "- predxs_dynamic: same as gtxs, but for prediction with correction factor (our approach). The difference with respect to gtxs is only on position of objects.\n",
    "\n",
    "\n",
    "- createdimages_original: a list, each element corresponds to a token (image). For each element, there is a dictionary. The dictionary contains: \n",
    "    - heat_gt: ground truth  trajectory, with 16 different trajectory poses, from t+0.25 until t+4.0 seconds. In practise, each of the 16 trajectory is conserved in a list of two elements: a tensor [256, 256] and an array [256,256,4]. For visualization, the array is better (just use showimg and a coloured picture appears), but the data is the same.\n",
    "    - heat_pt:  predicted trajectory. 16 different trajectory poses, from t+0.25 until t+4.0 seconds. Structure is as above.\n",
    "    - i: counter of the token (sample) processed\n",
    "    - gtx: equivalent of gtxs for a single specific token\n",
    "    - ptx: equivalent of predxs_original for a single specific token\n",
    "\n",
    "- createdimages_dynamic: a list, each element corresponds to a token (image). For each element, there is a dictionary. The dictionary contains: (as above but for pkl with correction factor)\n",
    "    - heat_gt: ground truth  trajectory. 16 different trajectory poses, from t+0.25 until t+4.0 seconds\n",
    "    - heat_pt:  predicted trajectory. 16 different trajectory poses, from t+0.25 until t+4.0 seconds\n",
    "    - i: counter of the token (sample) processed\n",
    "    - gtx: equivalent of gtxs for a single specific token\n",
    "    - ptx: equivalent of predxs_dynamic for a single specific token\n",
    "\n",
    "Also, there is a saved image as jpg, but the information of the image is contained in the above values. Just ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1061b69",
   "metadata": {},
   "source": [
    "**k frame where original function has maximum pkl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92713b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "number_of_trajectory_poses=16\n",
    "TRAJ_POINTS=25\n",
    "font=8\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "images_to_show=[item[0] for item in original_max]\n",
    "\n",
    "#first we consider worst cases with original pkl\n",
    "pklfile_original, info_original, all_pkls_original, gtdist,preddist_original,gtxs, predxs_original, \\\n",
    "createdimages_original, gt_boxes,nusc_maps=dt.calc_sample_crit_GOAL3_visualization(listtoken=images_to_show,\n",
    "                                     conf_th=basic_threshold,\n",
    "                                     crit=basic_criticality,\n",
    "                                     correction_factor=None, #in this case, it is just the normal pkl\n",
    "                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                     image_counter=len(images_to_show),\n",
    "                                     verbose=verbose);\n",
    "\n",
    "pklfile_dynamic, info_dynamic, all_pkls_dynamic, gtdist,preddist_dynamic,gtxs, predxs_dynamic, \\\n",
    "createdimages_dynamic, gt_boxes, nusc_maps=dt.calc_sample_crit_GOAL3_visualization(listtoken=images_to_show,\n",
    "                                     conf_th=basic_threshold,\n",
    "                                     crit=basic_criticality,\n",
    "                                     correction_factor=analyzed_correction_factor, #in this case, it is just the normal pkl\n",
    "                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                     image_counter=len(images_to_show),\n",
    "                                     verbose=verbose);\n",
    "number_of_trajectory_poses=16\n",
    "TRAJ_POINTS=25\n",
    "font=8\n",
    "\n",
    "%matplotlib inline\n",
    "for token_original, token_dynamic in zip(createdimages_original,createdimages_dynamic):\n",
    "    #get the ground truth bboxes\n",
    "    sGT=token_original['heat_gt']\n",
    "    ground_truths=token_original['gtx'].cpu().detach().numpy()[3]\n",
    "    ego=token_original['gtx'].cpu().detach().numpy()[4]\n",
    "    \n",
    "    #get the ground truth trajectory\n",
    "    trajectory_gt=computetrajectory(sGT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    \n",
    "    #get the predicted original bboxes\n",
    "    sPT=token_original['heat_pt']\n",
    "    original_detections=token_original['ptx'].cpu().detach().numpy()[3]\n",
    "    \n",
    "    #get the predicted original trajectories\n",
    "    trajectory_pt=computetrajectory(sPT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    \n",
    "    #get the predicted dynamic (our approach) bboxes\n",
    "    sDT=token_dynamic['heat_pt']\n",
    "    dynamic_detections=token_dynamic['ptx'].cpu().detach().numpy()[3]\n",
    "\n",
    "    #get the predicted dynamic (our approach) trajectories\n",
    "    trajectory_dt=computetrajectory(sDT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    printfigure3x2(font,#font\n",
    "                   trajectory_gt,#trajectory ground truth\n",
    "                   trajectory_pt,#trajectory predicted by original pkl\n",
    "                   trajectory_dt,#trajectory predicted by our approach to pkl\n",
    "                   ground_truths,#ground truth position of objects\n",
    "                   original_detections,#position of objects detected by original approach\n",
    "                   dynamic_detections,#position of objects detected by our approach (threshold filtering)\n",
    "                   ego\n",
    "                  )\n",
    "\n",
    "''':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb76bf",
   "metadata": {},
   "source": [
    "**k frame where dynamic function has maximum pkl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "images_to_show=[item[0] for item in dynamic_max]\n",
    "\n",
    "#first we consider worst cases with original pkl\n",
    "pklfile_original, info_original, all_pkls_original, gtdist,preddist_original,gtxs, predxs_original, \\\n",
    "createdimages_original, gt_boxes, nusc_maps=dt.calc_sample_crit_GOAL3_visualization(listtoken=images_to_show,\n",
    "                                     conf_th=basic_threshold,\n",
    "                                     crit=basic_criticality,\n",
    "                                     correction_factor=None, #in this case, it is just the normal pkl\n",
    "                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                     image_counter=len(images_to_show),\n",
    "                                     verbose=verbose);\n",
    "\n",
    "pklfile_dynamic, info_dynamic, all_pkls_dynamic, gtdist,preddist_dynamic,gtxs, predxs_dynamic, \\\n",
    "createdimages_dynamic, gt_boxes, nusc_maps=dt.calc_sample_crit_GOAL3_visualization(listtoken=images_to_show,\n",
    "                                     conf_th=basic_threshold,\n",
    "                                     crit=basic_criticality,\n",
    "                                     correction_factor=analyzed_correction_factor, #in this case, it is just the normal pkl\n",
    "                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                     image_counter=len(images_to_show),\n",
    "                                     verbose=verbose);\n",
    "\n",
    "%matplotlib inline\n",
    "for token_original, token_dynamic in zip(createdimages_original,createdimages_dynamic):\n",
    "    #get the ground truth bboxes\n",
    "    sGT=token_original['heat_gt']\n",
    "    ground_truths=token_original['gtx'].cpu().detach().numpy()[3]\n",
    "    ego=token_original['gtx'].cpu().detach().numpy()[4]\n",
    "    #ground_truths=np.swapaxes(ground_truths,0,1)\n",
    "    \n",
    "    #get the ground truth trajectory\n",
    "    trajectory_gt=computetrajectory(sGT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    #trajectory_gt=np.swapaxes(trajectory_gt, 0,1)   \n",
    "    \n",
    "    #get the predicted original bboxes\n",
    "    sPT=token_original['heat_pt']\n",
    "    original_detections=token_original['ptx'].cpu().detach().numpy()[3]\n",
    "    #original_detections=np.swapaxes(original_detections,0,1)\n",
    "    \n",
    "    #get the predicted original trajectories\n",
    "    trajectory_pt=computetrajectory(sPT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    #trajectory_pt=np.swapaxes(trajectory_pt, 0,1)\n",
    "    \n",
    "    #get the predicted dynamic (our approach) bboxes\n",
    "    sDT=token_dynamic['heat_pt']\n",
    "    dynamic_detections=token_dynamic['ptx'].cpu().detach().numpy()[3]\n",
    "    #dynamic_detections=np.swapaxes(dynamic_detections,0,1)\n",
    "\n",
    "    #get the predicted dynamic (our approach) trajectories\n",
    "    trajectory_dt=computetrajectory(sDT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    #trajectory_dt=np.swapaxes(trajectory_dt, 0,1)\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    printfigure3x2(font,#font\n",
    "                   trajectory_gt,#trajectory ground truth\n",
    "                   trajectory_pt,#trajectory predicted by original pkl\n",
    "                   trajectory_dt,#trajectory predicted by our approach to pkl\n",
    "                   ground_truths,#ground truth position of objects\n",
    "                   original_detections,#position of objects detected by original approach\n",
    "                   dynamic_detections,#position of objects detected by our approach (threshold filtering)\n",
    "                   ego\n",
    "                  )\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49a37f1",
   "metadata": {},
   "source": [
    "**n frame with maximum difference between pkl with original function and our modified function (i.e., our modified function is closer to the ground truth)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "images_to_show=[item[0] for item in max_gain_d_over_o]\n",
    "\n",
    "#first we consider worst cases with original pkl\n",
    "pklfile_original, info_original, all_pkls_original, gtdist,preddist_original,gtxs, predxs_original, \\\n",
    "createdimages_original, gt_boxes, nusc_maps=dt.calc_sample_crit_GOAL3_visualization(listtoken=images_to_show,\n",
    "                                     conf_th=basic_threshold,\n",
    "                                     crit=basic_criticality,\n",
    "                                     correction_factor=None, #in this case, it is just the normal pkl\n",
    "                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                     image_counter=len(images_to_show),\n",
    "                                     verbose=verbose);\n",
    "\n",
    "pklfile_dynamic, info_dynamic, all_pkls_dynamic, gtdist,preddist_dynamic,gtxs, predxs_dynamic, \\\n",
    "createdimages_dynamic, gt_boxes, nusc_maps=dt.calc_sample_crit_GOAL3_visualization(listtoken=images_to_show,\n",
    "                                     conf_th=basic_threshold,\n",
    "                                     crit=basic_criticality,\n",
    "                                     correction_factor=analyzed_correction_factor, #in this case, it is just the normal pkl\n",
    "                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                     image_counter=len(images_to_show),\n",
    "                                     verbose=verbose);\n",
    "\n",
    "%matplotlib inline\n",
    "for token_original, token_dynamic in zip(createdimages_original, createdimages_dynamic):\n",
    "    #get the ground truth bboxes\n",
    "    sGT=token_original['heat_gt']\n",
    "    ground_truths=token_original['gtx'].cpu().detach().numpy()[3]\n",
    "    ego=token_original['gtx'].cpu().detach().numpy()[4]\n",
    "    #ground_truths=np.swapaxes(ground_truths,0,1)\n",
    "    \n",
    "    #get the ground truth trajectory\n",
    "    trajectory_gt=computetrajectory(sGT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    #trajectory_gt=np.swapaxes(trajectory_gt, 0,1)   \n",
    "    \n",
    "    #get the predicted original bboxes\n",
    "    sPT=token_original['heat_pt']\n",
    "    original_detections=token_original['ptx'].cpu().detach().numpy()[3]\n",
    "    #original_detections=np.swapaxes(original_detections,0,1)\n",
    "    \n",
    "    #get the predicted original trajectories\n",
    "    trajectory_pt=computetrajectory(sPT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    #trajectory_pt=np.swapaxes(trajectory_pt, 0,1)\n",
    "    \n",
    "    #get the predicted dynamic (our approach) bboxes\n",
    "    sDT=token_dynamic['heat_pt']\n",
    "    dynamic_detections=token_dynamic['ptx'].cpu().detach().numpy()[3]\n",
    "    #dynamic_detections=np.swapaxes(dynamic_detections,0,1)\n",
    "\n",
    "    #get the predicted dynamic (our approach) trajectories\n",
    "    trajectory_dt=computetrajectory(sDT.copy(), number_of_trajectory_poses, TRAJ_POINTS)\n",
    "    #trajectory_dt=np.swapaxes(trajectory_dt, 0,1)\n",
    "\n",
    "    plt.clf()\n",
    "    printfigure3x2(font,#font\n",
    "                   trajectory_gt,#trajectory ground truth\n",
    "                   trajectory_pt,#trajectory predicted by original pkl\n",
    "                   trajectory_dt,#trajectory predicted by our approach to pkl\n",
    "                   ground_truths,#ground truth position of objects\n",
    "                   original_detections,#position of objects detected by original approach\n",
    "                   dynamic_detections,#position of objects detected by our approach (threshold filtering)\n",
    "                   ego\n",
    "                  )\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ba792",
   "metadata": {},
   "source": [
    "**Understand if there are safety violations**\n",
    "\n",
    "*We want to identify specific cases, to understand what would happen, and, especially, if we improve safety, reducing the risk of crashes.*\n",
    "\n",
    "To proceed on this, we check if there are some \"unsafe trajectory\". We are inspired by the rule of the 3-second distance, according to the US National Authorities [1], [2]: this rule establishes that you are driving with a safe distance if it would take you 3 seconds to reach the position of the vehicle in front of you (i.e., supposed the vehicle in front freezes instantly, it would take 3 seconds to bump into it).\n",
    "\n",
    "[1] Reference Material for DDC Instructors, 5th Edition. National Safety Council, 2005.\n",
    "\n",
    "[2] https://www.travelers.com/resources/auto/travel/3-second-rule-for-safe-following-distance\n",
    "\n",
    "\n",
    "Inspired by this, we consider the 4 seconds trajectory planned by pkl. We check the projected trajectories, and we see if the projected position of the ego vehicle in 4 seconds will lead to an overlap with the ground truth objects projected in the next 4 seconds.\n",
    "\n",
    "For this, we first select the relevant frames. Realistically we should select frames which represent the most critical scenarios. However, it is difficult to say this from pkl, as it is mostly a reliability metric rather than a safety metric, as demonstrated in the SAC paper we published recently. So, pin-pointing a special frame (maybe the one which shows the best pkl improvement, or the one that mostly reduces the maximum pkl) is good only for visualization purposes.\n",
    "\n",
    "We want to plot \"up to 4 seconds\": we do not apply on the 20 seconds length, but only on the first 12 seconds of each sequence.\n",
    "\n",
    "We see if the projected position of the ego vehicle in 4 seconds will lead to an overlap with the ground truth objects.\n",
    "\n",
    "- For the ground truth pkl\n",
    "- the pkl using the bbox predicted with the original function.\n",
    "- the pkl using the bbox predicted with our modified function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b22d52",
   "metadata": {},
   "source": [
    "we first compute pkls for all val set considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ddb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alltoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e417fd2",
   "metadata": {},
   "source": [
    "we retrieve the nuscene scenes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve initial token of each nuscene scene in val\n",
    "#Elements as:\n",
    "#{'token': '41fde20fedcd4d22ab26811688612870',\n",
    "#  'log_token': 'd31dc715d1c34b99bd5afb0e3aea26ed',\n",
    "#  'nbr_samples': 40,\n",
    "#  'first_sample_token': 'b9c60cfaf1814c8bb363037dec7abd35',\n",
    "#  'last_sample_token': '51f7ea7d3e4e42dfaf183dbe9996f1fb',\n",
    "#  'name': 'scene-0013',\n",
    "#  'description': 'Follow bus, parked cars, trash cans, parked bicycles, nature'},\n",
    "\n",
    "nuscenes_sequences=[]\n",
    "\n",
    "for i in nuscenes.scene:\n",
    "    if (i['name'] in val):\n",
    "        nuscenes_sequences.append(i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834497a8",
   "metadata": {},
   "source": [
    "for each token, we compute the successive 8 and we perform the \"do_computation\" that does essentially everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ca86b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#get the related sequence, to compute predictions number of images we have used in our validation\n",
    "#first we get the my_sample with the reference selected_token\n",
    "escape=0\n",
    "dx,bx, (nx, ny) = get_grid([-17.0, -38.5, 60.0,38.5], [0.3, 0.3])\n",
    "#dx,bx, (nx, ny) = get_grid([-34.0, -77, 120.0,77], [0.5, 0.5])\n",
    "stretch=70 #120\n",
    "layer_names = ['road_segment', 'lane']\n",
    "line_names = ['road_divider', 'lane_divider']\n",
    "\n",
    "for sequence in nuscenes_sequences:\n",
    "    sequence_length=sequence['nbr_samples']\n",
    "    frames_can_use=sequence_length-(seconds*2) #remove the last 4 seconds, that are 8 frames\n",
    "    current_frame=0\n",
    "    token  = sequence['first_sample_token']\n",
    "    while(current_frame < frames_can_use):\n",
    "        my_selected_sample = nuscenes.get('sample', token)\n",
    "        tokens=get_following_tokens(seconds, token, my_selected_sample)\n",
    "        \n",
    "        pklfile_original, info_original, all_pkls_original, gtdist,preddist_original,gtxs, predxs_original, \\\n",
    "        createdimages_original, gt_boxes, nusc_maps=dt.calc_sample_crit_GOAL3_visualization(listtoken=tokens,\n",
    "                                     conf_th=basic_threshold,\n",
    "                                     crit=basic_criticality,\n",
    "                                     correction_factor=None, #in this case, it is just the normal pkl\n",
    "                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                     image_counter=len(tokens),\n",
    "                                     verbose=verbose);\n",
    "\n",
    "        pklfile_dynamic, info_dynamic, all_pkls_dynamic, gtdist,preddist_dynamic,gtxs, predxs_dynamic, \\\n",
    "        createdimages_dynamic, gt_boxes, nusc_maps=dt.calc_sample_crit_GOAL3_visualization(listtoken=tokens,\n",
    "                                     conf_th=basic_threshold,\n",
    "                                     crit=basic_criticality,\n",
    "                                     correction_factor=analyzed_correction_factor, #in this case, it is just the normal pkl\n",
    "                                     obj_classes_list=object_classes,#filter boxes based on class\n",
    "                                     image_counter=len(tokens),\n",
    "                                     verbose=verbose);\n",
    "\n",
    "        #for the trajectory with the original and dynamic approach\n",
    "        do_computation(tokens,\n",
    "                            info_original, createdimages_original, \n",
    "                            info_dynamic, createdimages_dynamic,\n",
    "                            dx,bx, nx, ny,stretch, \n",
    "                            nuscenes, nusc_maps, gt_boxes )\n",
    "        \n",
    "        current_frame=current_frame+1;\n",
    "        token=my_selected_sample['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a352f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp='/home/notebook/pkl/results/GOAL3/SSN/'\n",
    "tken='62cc9dae3ca2420fa3629271cdd4212b_GT BBoxes and GT trajectory.npy'\n",
    "a=np.load(pp+tken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf479730",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.imshow(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bfb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04d768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "openmmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
